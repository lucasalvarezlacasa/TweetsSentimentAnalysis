{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis - Logistic Regression (Deep Learning)\n",
    "In this small POC, I will show you how you can apply logistic regression in order to determine if a given tweet has a positive or a negative sentiment behind it.\n",
    "For this, we will use a dataset included in NLTK package that already contains 10k labeled tweets, 5k of them marked as positive and the other 5k as negative. This is really good, since we will have a perfect distribution of tweet so our model can learn better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data and understanding it\n",
    "Our first step is always getting along with the data, we need to understand what is the format of it and how we can obtain the information that we need to train our model with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of positive tweets: 5000\n",
      "Amount of negative tweets: 5000\n",
      "\n",
      "Some positive tweets:\n",
      "['@Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!', '@DespiteOfficial we had a listen last night :) As You Bleed is an amazing track. When are you in Scotland?!', '@97sides CONGRATS :)', 'yeaaaah yippppy!!!  my accnt verified rqst has succeed got a blue tick mark on my fb profile :) in 15 days', '@BhaktisBanter @PallaviRuhail This one is irresistible :)\\n#FlipkartFashionFriday http://t.co/EbZ0L2VENM', \"We don't like to keep our lovely customers waiting for long! We hope you enjoy! Happy Friday! - LWWF :) https://t.co/smyYriipxI\", '@Impatientraider On second thought, there’s just not enough time for a DD :) But new shorts entering system. Sheep must be buying.', 'Jgh , but we have to go to Bayan :D bye', 'As an act of mischievousness, am calling the ETL layer of our in-house warehousing app Katamari.\\n\\nWell… as the name implies :p.']\n",
      "\n",
      "Some negative tweets:\n",
      "[\"Everything in the kids section of IKEA is so cute. Shame I'm nearly 19 in 2 months :(\", '@Hegelbon That heart sliding into the waste basket. :(', '“@ketchBurning: I hate Japanese call him \"bani\" :( :(”\\n\\nMe too', 'Dang starting next week I have \"work\" :(', \"oh god, my babies' faces :( https://t.co/9fcwGvaki0\", '@RileyMcDonough make me smile :((', '@f0ggstar @stuartthull work neighbour on motors. Asked why and he said hates the updates on search :( http://t.co/XvmTUikWln', 'why?:(\"@tahuodyy: sialan:( https://t.co/Hv1i0xcrL2\"', 'Athabasca glacier was there in #1948 :-( #athabasca #glacier #jasper #jaspernationalpark #alberta #explorealberta #… http://t.co/dZZdqmf7Cz']\n"
     ]
    }
   ],
   "source": [
    "# Import some libraries we always use\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk.corpus import twitter_samples\n",
    "\n",
    "\n",
    "# To know the ids of the json files, we can run the following command\n",
    "#print(twitter_samples.fileids())\n",
    "\n",
    "# Reading our files\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "\n",
    "# Count how many type of tweets we have in each case\n",
    "print(f\"Amount of positive tweets: {len(all_positive_tweets)}\")\n",
    "print(f\"Amount of negative tweets: {len(all_negative_tweets)}\")\n",
    "print()\n",
    "\n",
    "# Let's display some samples\n",
    "print(\"Some positive tweets:\")\n",
    "print(all_positive_tweets[1:10])\n",
    "print()\n",
    "print(\"Some negative tweets:\")\n",
    "print(all_negative_tweets[1:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing and cleaning our data\n",
    "This is a classic step of the NLP pipeline. In here, we will proceed to clean the text, remove punctuation, tokenize it into separate words, remove stopwords (or most common words in English) and transform them into their root version (stemming).\n",
    "\n",
    "For this particular case of tweets, we can see that there are some URLS inside of the message that we should also get rid of (since URLS don't add any value in the sentiment of a message). Same thing happens with quotes or tags (we can see in the previos messages some mentions to users such as \"@ketchBurning\" which won't add any sentiment either).\n",
    "Let's get rid of all this information and clean our text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Tweet: #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
      "Processed Tweet: ['followfriday', 'top', 'engag', 'member', 'commun', 'week', ':)']\n"
     ]
    }
   ],
   "source": [
    "import re                                  # library for regular expression operations\n",
    "import string                              # for string operations\n",
    "\n",
    "from nltk.corpus import stopwords          # module for stop words that come with NLTK\n",
    "from nltk.stem import PorterStemmer        # module for stemming\n",
    "from nltk.tokenize import TweetTokenizer   # module for tokenizing strings\n",
    "\n",
    "ps = PorterStemmer()\n",
    "en_stopwords = stopwords.words('english')\n",
    "\n",
    "def process_tweet(tweet):\n",
    "    # Remove hashtag, retweet marks, and hyperlinks\n",
    "    # Remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "\n",
    "    # Remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "\n",
    "    # Remove hashtags\n",
    "    # Only removing the hash # sign from the word\n",
    "    tweet = re.sub(r'#', '', tweet)  \n",
    "    \n",
    "    # Tokenize and lowercase words so (\"Hello\" and \"hello\" have the same meaning)\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,reduce_len=True)\n",
    "    tweet_tokenized = tokenizer.tokenize(tweet)\n",
    "    \n",
    "    # Remove punctuation, stopwords and stem token\n",
    "    tweet_tokenized_cleaned = []\n",
    "    for token in tweet_tokenized:\n",
    "        if token not in string.punctuation and token not in en_stopwords:\n",
    "            tweet_tokenized_cleaned.append(ps.stem(token))\n",
    "    \n",
    "    return tweet_tokenized_cleaned\n",
    "    \n",
    "\n",
    "# Testing it out\n",
    "sample_tweet = all_positive_tweets[0]\n",
    "processed_tweet = process_tweet(sample_tweet)\n",
    "print(f\"Sample Tweet: {sample_tweet}\")\n",
    "print(f\"Processed Tweet: {processed_tweet}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What can we use to train our model with?\n",
    "Well, we now have the capability of processing each one of our tweets. However, we need to think what type of information we're going to use in our model in order for it to determine if a given token has a positive or a negative sentiment.\n",
    "\n",
    "One thing that is clear is that we want a vector representation for each one of our tweets, and we want that representation be as optimal as possible. This is called feature extraction.\n",
    "What we could do in this case is start by counting how many times each word appears in possitive tweets and how many times it appears in negative ones. We will then generate a method that can give us a dictionary with (word,label) as keys and then as value we will have the total times that word appeared in messages with that label.\n",
    "\n",
    "As an example, we might have:\n",
    "(\"week\", 1) -> 15\n",
    "(\"week\", 0) -> 5\n",
    "Meaning that the token \"week\" appeared 15 times in possitive messages (that's why we use 1 as part of the key) and 5 times in tweets labeled as negative. We will use this dictionary to later build our features vector (this is, the vector we will use as input to our model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('followfriday', 1.0): 1, ('top', 1.0): 1, ('engag', 1.0): 1, ('member', 1.0): 1, ('commun', 1.0): 1, ('week', 1.0): 1, (':)', 1.0): 8, ('hey', 1.0): 1, ('jame', 1.0): 1, ('odd', 1.0): 1, (':/', 1.0): 1, ('pleas', 1.0): 1, ('call', 1.0): 2, ('contact', 1.0): 1, ('centr', 1.0): 1, ('02392441234', 1.0): 1, ('abl', 1.0): 1, ('assist', 1.0): 1, ('mani', 1.0): 1, ('thank', 1.0): 1, ('listen', 1.0): 1, ('last', 1.0): 1, ('night', 1.0): 1, ('bleed', 1.0): 1, ('amaz', 1.0): 1, ('track', 1.0): 1, ('scotland', 1.0): 1, ('congrat', 1.0): 1, ('yeaaah', 1.0): 1, ('yipppi', 1.0): 1, ('accnt', 1.0): 1, ('verifi', 1.0): 1, ('rqst', 1.0): 1, ('succeed', 1.0): 1, ('got', 1.0): 1, ('blue', 1.0): 1, ('tick', 1.0): 1, ('mark', 1.0): 1, ('fb', 1.0): 1, ('profil', 1.0): 1, ('15', 1.0): 1, ('day', 1.0): 1, ('one', 1.0): 1, ('irresist', 1.0): 1, ('flipkartfashionfriday', 1.0): 1, ('like', 1.0): 1, ('keep', 1.0): 1, ('love', 1.0): 1, ('custom', 1.0): 1, ('wait', 1.0): 1, ('long', 1.0): 1, ('hope', 1.0): 1, ('enjoy', 1.0): 1, ('happi', 1.0): 1, ('friday', 1.0): 1, ('lwwf', 1.0): 1, ('second', 1.0): 1, ('thought', 1.0): 1, ('’', 1.0): 1, ('enough', 1.0): 1, ('time', 1.0): 1, ('dd', 1.0): 1, ('new', 1.0): 1, ('short', 1.0): 1, ('enter', 1.0): 1, ('system', 1.0): 1, ('sheep', 1.0): 1, ('must', 1.0): 1, ('buy', 1.0): 1, ('jgh', 1.0): 1, ('go', 1.0): 1, ('bayan', 1.0): 1, (':D', 1.0): 1, ('bye', 1.0): 1, ('act', 1.0): 1, ('mischiev', 1.0): 1, ('etl', 1.0): 1, ('layer', 1.0): 1, ('in-hous', 1.0): 1, ('wareh', 1.0): 1, ('app', 1.0): 1, ('katamari', 1.0): 1, ('well', 1.0): 1, ('…', 1.0): 1, ('name', 1.0): 1, ('impli', 1.0): 1, (':p', 1.0): 1, ('hopeless', 0.0): 1, ('tmr', 0.0): 1, (':(', 0.0): 11, ('everyth', 0.0): 1, ('kid', 0.0): 1, ('section', 0.0): 1, ('ikea', 0.0): 1, ('cute', 0.0): 1, ('shame', 0.0): 1, (\"i'm\", 0.0): 1, ('nearli', 0.0): 1, ('19', 0.0): 1, ('2', 0.0): 1, ('month', 0.0): 1, ('heart', 0.0): 1, ('slide', 0.0): 1, ('wast', 0.0): 1, ('basket', 0.0): 1, ('“', 0.0): 1, ('hate', 0.0): 2, ('japanes', 0.0): 1, ('call', 0.0): 1, ('bani', 0.0): 1, ('”', 0.0): 1, ('dang', 0.0): 1, ('start', 0.0): 1, ('next', 0.0): 1, ('week', 0.0): 1, ('work', 0.0): 2, ('oh', 0.0): 1, ('god', 0.0): 1, ('babi', 0.0): 1, ('face', 0.0): 1, ('make', 0.0): 1, ('smile', 0.0): 1, ('neighbour', 0.0): 1, ('motor', 0.0): 1, ('ask', 0.0): 1, ('said', 0.0): 1, ('updat', 0.0): 1, ('search', 0.0): 1, ('sialan', 0.0): 1, ('athabasca', 0.0): 2, ('glacier', 0.0): 2, ('1948', 0.0): 1, (':-(', 0.0): 1, ('jasper', 0.0): 1, ('jaspernationalpark', 0.0): 1, ('alberta', 0.0): 1, ('explorealberta', 0.0): 1, ('…', 0.0): 1}\n"
     ]
    }
   ],
   "source": [
    "# as input, this will receive all the tweets and labels (1 or 0) we use to train our model with\n",
    "def frequency_dictionary(tweets,labels):  \n",
    "    # I will zip the tweets and the labels so we can get a tuple representation of each tweet and his value\n",
    "    tweets_labels = zip(tweets,labels)\n",
    "    freq_dict = {}\n",
    "    \n",
    "    for (tweet,label) in tweets_labels:\n",
    "        tweet_tokens = process_tweet(tweet)\n",
    "        for token in tweet_tokens:\n",
    "            pair = (token,label)\n",
    "            if pair in freq_dict:\n",
    "                freq_dict[pair] +=1\n",
    "            else:\n",
    "                freq_dict[pair] = 1\n",
    "    \n",
    "    return freq_dict\n",
    "                \n",
    "# Let's try this with 10 messages from each type of tweet\n",
    "sample_msgs = all_positive_tweets[:10] + all_negative_tweets[:10]\n",
    "sample_labels = np.append(np.ones((10,1)),np.zeros((10,1)))\n",
    "sample_freq_dict = frequency_dictionary(sample_msgs,sample_labels)\n",
    "print(sample_freq_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweet representation as a vector\n",
    "Since we will be using a deep learning model in this example, having a sparse matrix to represent each tweet is not the best choice. A sparse matrix is simply a matrix that contains as columns all the words being across all different tweets that we use to train our model with (this is called the vocabulary) and as rows each one of those messages. As values, you will have a 1 or a 0 if that word is being used in the given tweet or not.\n",
    "The problem is that this matrix having 1's and 0's can be really big, and our model would have to learn a lot of parameters in order for it to be able to predict later on if a given tweet has a negative or positive sentiment.\n",
    "\n",
    "Instead of this representation, we will represent each one of our tweets as a vector of 3 dimensions:\n",
    "<b>[bias, sum of positive frequencies for each token in that tweet, sum of negative frequencies for each token in that tweet]</b>. \n",
    "Instead of learning \"v\" features, we will have to learn only 3 features.\n",
    "\n",
    "Let's define a function that can help us extracting those features and generating the row vector that we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(tweet, freqs):\n",
    "    tweet_representation = np.zeros((1,3))\n",
    "    tweet_representation[0,0] = 1\n",
    "    \n",
    "    pos_sum = 0\n",
    "    neg_sum = 0\n",
    "\n",
    "    for token in process_tweet(tweet):        \n",
    "        # If the key is not found, it will return a 0, adding nothing to our sum\n",
    "        tweet_representation[0,1] += freqs.get((token,1.0), 0)\n",
    "        tweet_representation[0,2] += freqs.get((token,0.0), 0)\n",
    "   \n",
    "    return tweet_representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('followfriday', 1.0): 1, ('top', 1.0): 1, ('engag', 1.0): 1, ('member', 1.0): 1, ('commun', 1.0): 1, ('week', 1.0): 1, (':)', 1.0): 8, ('hey', 1.0): 1, ('jame', 1.0): 1, ('odd', 1.0): 1, (':/', 1.0): 1, ('pleas', 1.0): 1, ('call', 1.0): 2, ('contact', 1.0): 1, ('centr', 1.0): 1, ('02392441234', 1.0): 1, ('abl', 1.0): 1, ('assist', 1.0): 1, ('mani', 1.0): 1, ('thank', 1.0): 1, ('listen', 1.0): 1, ('last', 1.0): 1, ('night', 1.0): 1, ('bleed', 1.0): 1, ('amaz', 1.0): 1, ('track', 1.0): 1, ('scotland', 1.0): 1, ('congrat', 1.0): 1, ('yeaaah', 1.0): 1, ('yipppi', 1.0): 1, ('accnt', 1.0): 1, ('verifi', 1.0): 1, ('rqst', 1.0): 1, ('succeed', 1.0): 1, ('got', 1.0): 1, ('blue', 1.0): 1, ('tick', 1.0): 1, ('mark', 1.0): 1, ('fb', 1.0): 1, ('profil', 1.0): 1, ('15', 1.0): 1, ('day', 1.0): 1, ('one', 1.0): 1, ('irresist', 1.0): 1, ('flipkartfashionfriday', 1.0): 1, ('like', 1.0): 1, ('keep', 1.0): 1, ('love', 1.0): 1, ('custom', 1.0): 1, ('wait', 1.0): 1, ('long', 1.0): 1, ('hope', 1.0): 1, ('enjoy', 1.0): 1, ('happi', 1.0): 1, ('friday', 1.0): 1, ('lwwf', 1.0): 1, ('second', 1.0): 1, ('thought', 1.0): 1, ('’', 1.0): 1, ('enough', 1.0): 1, ('time', 1.0): 1, ('dd', 1.0): 1, ('new', 1.0): 1, ('short', 1.0): 1, ('enter', 1.0): 1, ('system', 1.0): 1, ('sheep', 1.0): 1, ('must', 1.0): 1, ('buy', 1.0): 1, ('jgh', 1.0): 1, ('go', 1.0): 1, ('bayan', 1.0): 1, (':D', 1.0): 1, ('bye', 1.0): 1, ('act', 1.0): 1, ('mischiev', 1.0): 1, ('etl', 1.0): 1, ('layer', 1.0): 1, ('in-hous', 1.0): 1, ('wareh', 1.0): 1, ('app', 1.0): 1, ('katamari', 1.0): 1, ('well', 1.0): 1, ('…', 1.0): 1, ('name', 1.0): 1, ('impli', 1.0): 1, (':p', 1.0): 1, ('hopeless', 0.0): 1, ('tmr', 0.0): 1, (':(', 0.0): 11, ('everyth', 0.0): 1, ('kid', 0.0): 1, ('section', 0.0): 1, ('ikea', 0.0): 1, ('cute', 0.0): 1, ('shame', 0.0): 1, (\"i'm\", 0.0): 1, ('nearli', 0.0): 1, ('19', 0.0): 1, ('2', 0.0): 1, ('month', 0.0): 1, ('heart', 0.0): 1, ('slide', 0.0): 1, ('wast', 0.0): 1, ('basket', 0.0): 1, ('“', 0.0): 1, ('hate', 0.0): 2, ('japanes', 0.0): 1, ('call', 0.0): 1, ('bani', 0.0): 1, ('”', 0.0): 1, ('dang', 0.0): 1, ('start', 0.0): 1, ('next', 0.0): 1, ('week', 0.0): 1, ('work', 0.0): 2, ('oh', 0.0): 1, ('god', 0.0): 1, ('babi', 0.0): 1, ('face', 0.0): 1, ('make', 0.0): 1, ('smile', 0.0): 1, ('neighbour', 0.0): 1, ('motor', 0.0): 1, ('ask', 0.0): 1, ('said', 0.0): 1, ('updat', 0.0): 1, ('search', 0.0): 1, ('sialan', 0.0): 1, ('athabasca', 0.0): 2, ('glacier', 0.0): 2, ('1948', 0.0): 1, (':-(', 0.0): 1, ('jasper', 0.0): 1, ('jaspernationalpark', 0.0): 1, ('alberta', 0.0): 1, ('explorealberta', 0.0): 1, ('…', 0.0): 1}\n",
      "\n",
      "[[1. 8. 2.]]\n"
     ]
    }
   ],
   "source": [
    "print(sample_freq_dict)\n",
    "print()\n",
    "print(extract_features(\"I'm learning NLP :) alberta\",sample_freq_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression in practice\n",
    "Logistic regression is a simple form of a neural network that classifies data categorically. For example, classifying emails as spam or non-spam is a classic use case of logistic regression. So how does it work? Simple. Logistic regression takes an input, passes it through a function called sigmoid function (https://en.wikipedia.org/wiki/Sigmoid_function) then returns an output of probability between 0 and 1. \n",
    "This sigmoid function is responsible for classifying the input and in our case it will tell us the probability of a given tweet to be positive or negative.\n",
    "\n",
    "Now, we know that there is a high chance of a wrong classification by the sigmoid function, which is bad for the algorithm. This “mistake” is also known as weight or loss. The goal of a good logistic regression algorithm is to reduce loss or weight by improving the correctness of the output and this is achieved by a function called Gradient Descent. A good way to evaluate the performance of the logistic regression algorithm is by achieving a minimal cost function. Cost function quantifies the error between the predicted value and the expected values.\n",
    "\n",
    "In our case, the weights will be parameters that are going to affect our input parameters (I mentioned before we had a column vector of 3 elements that we need to learn) and the objective of gradient descent is to help us find those weights that are going to produce the minimum cost or error at the end of the function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression: regression and a sigmoid\n",
    "\n",
    "Logistic regression takes a regular linear regression, and applies a sigmoid to the output of the linear regression.\n",
    "\n",
    "Regression:\n",
    "$$z = \\theta_0 x_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... \\theta_N x_N$$\n",
    "Note that the $\\theta$ values are \"weights\".\n",
    "\n",
    "Logistic regression\n",
    "$$ h(z) = \\frac{1}{1+\\exp^{-z}}$$\n",
    "$$z = \\theta_0 x_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... \\theta_N x_N$$\n",
    "We will refer to 'z' as the 'logits'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function and Gradient\n",
    "\n",
    "The cost function used for logistic regression is the average of the log loss across all training examples:\n",
    "\n",
    "$$J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^m y^{(i)}\\log (h(z(\\theta)^{(i)})) + (1-y^{(i)})\\log (1-h(z(\\theta)^{(i)}))\\tag{5} $$\n",
    "* $m$ is the number of training examples\n",
    "* $y^{(i)}$ is the actual label of the i-th training example.\n",
    "* $h(z(\\theta)^{(i)})$ is the model's prediction for the i-th training example.\n",
    "\n",
    "The loss function for a single training example is\n",
    "$$ Loss = -1 \\times \\left( y^{(i)}\\log (h(z(\\theta)^{(i)})) + (1-y^{(i)})\\log (1-h(z(\\theta)^{(i)})) \\right)$$\n",
    "\n",
    "* All the $h$ values are between 0 and 1, so the logs will be negative. That is the reason for the factor of -1 applied to the sum of the two loss terms.\n",
    "* Note that when the model predicts 1 ($h(z(\\theta)) = 1$) and the label $y$ is also 1, the loss for that training example is 0.\n",
    "* Similarly, when the model predicts 0 ($h(z(\\theta)) = 0$) and the actual label is also 0, the loss for that training example is 0. \n",
    "* However, when the model prediction is close to 1 ($h(z(\\theta)) = 0.9999$) and the label is 0, the second term of the log loss becomes a large negative number, which is then multiplied by the overall factor of -1 to convert it to a positive loss value. $-1 \\times (1 - 0) \\times log(1 - 0.9999) \\approx 9.2$ The closer the model prediction gets to 1, the larger the loss. This happens the other way around as well (if the model prediction is close to 0 and the label is 1 the loss will generate a value that goes to infinity)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update the weights\n",
    "\n",
    "To update your weight vector $\\theta$, you will apply gradient descent to iteratively improve your model's predictions.  \n",
    "The gradient of the cost function $J$ with respect to one of the weights $\\theta_j$ is:\n",
    "\n",
    "$$\\nabla_{\\theta_j}J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m(h^{(i)}-y^{(i)})x_j \\tag{5}$$\n",
    "* 'i' is the index across all 'm' training examples.\n",
    "* 'j' is the index of the weight $\\theta_j$, so $x_j$ is the feature associated with weight $\\theta_j$\n",
    "\n",
    "* To update the weight $\\theta_j$, we adjust it by subtracting a fraction of the gradient determined by $\\alpha$:\n",
    "$$\\theta_j = \\theta_j - \\alpha \\times \\nabla_{\\theta_j}J(\\theta) $$\n",
    "* The learning rate $\\alpha$ is a value that we choose to control how big a single update will be.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement gradient descent function\n",
    "* The number of iterations `num_iters` is the number of times that you'll use the entire training set.\n",
    "* For each iteration, you'll calculate the cost function using all training examples (there are `m` training examples), and for all features.\n",
    "* Instead of updating a single weight $\\theta_i$ at a time, we can update all the weights in the column vector:  \n",
    "$$\\mathbf{\\theta} = \\begin{pmatrix}\n",
    "\\theta_0\n",
    "\\\\\n",
    "\\theta_1\n",
    "\\\\ \n",
    "\\theta_2 \n",
    "\\\\ \n",
    "\\vdots\n",
    "\\\\ \n",
    "\\theta_n\n",
    "\\end{pmatrix}$$\n",
    "* $\\mathbf{\\theta}$ has dimensions (n+1, 1), where 'n' is the number of features, and there is one more element for the bias term $\\theta_0$ (note that the corresponding feature value $\\mathbf{x_0}$ is 1).\n",
    "* The 'logits', 'z', are calculated by multiplying the feature matrix 'x' with the weight vector 'theta'.  $z = \\mathbf{x}\\mathbf{\\theta}$\n",
    "    * $\\mathbf{x}$ has dimensions (m, n+1) -> In our case n = 2 (the sum of positive freqs for each token in the tweet and the sum of negative freqs for each token in the tweet).\n",
    "    * $\\mathbf{\\theta}$: has dimensions (n+1, 1)\n",
    "    * $\\mathbf{z}$: has dimensions (m, 1) -> Makes since, we have one predicted value per example (we have \"m\" examples)\n",
    "* The prediction 'h', is calculated by applying the sigmoid to each element in 'z': $h(z) = sigmoid(z)$, and has dimensions (m,1).\n",
    "* The cost function $J$ is calculated by taking the dot product of the vectors 'y' and 'log(h)'.  Since both 'y' and 'h' are column vectors (m,1), transpose the vector to the left, so that matrix multiplication of a row vector with column vector performs the dot product.\n",
    "$$J = \\frac{-1}{m} \\times \\left(\\mathbf{y}^T \\cdot log(\\mathbf{h}) + \\mathbf{(1-y)}^T \\cdot log(\\mathbf{1-h}) \\right)$$\n",
    "* The update of theta is also vectorized.  Because the dimensions of $\\mathbf{x}$ are (m, n+1), and both $\\mathbf{h}$ and $\\mathbf{y}$ are (m, 1), we need to transpose the $\\mathbf{x}$ and place it on the left in order to perform matrix multiplication, which then yields the (n+1, 1) answer we need:\n",
    "$$\\mathbf{\\theta} = \\mathbf{\\theta} - \\frac{\\alpha}{m} \\times \\left( \\mathbf{x}^T \\cdot \\left( \\mathbf{h-y} \\right) \\right)$$\n",
    "\n",
    "\n",
    "<b>IMPORTANT: we vectorize these calculations in order to avoid for loops (otherwise we would need to loop on each training example in order to calculate the sum needed for the cost function)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(x,y,theta,learning_rate,num_iters):\n",
    "    '''\n",
    "    Input:\n",
    "        x: matrix of features which is (m,n+1)\n",
    "        y: corresponding labels of the input matrix x, dimensions (m,1)\n",
    "        theta: weight vector of dimension (n+1,1)\n",
    "        alpha: learning rate\n",
    "        num_iters: number of iterations you want to train your model for\n",
    "    Output:\n",
    "        J: the final cost\n",
    "        theta: your final weight vector\n",
    "    '''  \n",
    "    m = x.shape[0]\n",
    "    \n",
    "    for i in range(0,num_iters):\n",
    "        # This is also called as FORWARD PROPAGATION -------------------\n",
    "        # Calculate z\n",
    "        z = np.dot(x,theta)\n",
    "        \n",
    "        # Calculate h(z), this is the sigmoid function\n",
    "        h = sigmoid(z)\n",
    "                       \n",
    "        # This step is also called as BACKWARDS PROPAGATION ------------    \n",
    "        # Calculate the cost (this should go down in every iteration)\n",
    "        J = (-1/m)*(np.dot(y.T,np.log(h)) + np.dot((1-y).T,np.log(1-h)))\n",
    "        \n",
    "        # Update theta according to the GD formula        \n",
    "        theta = theta - (learning_rate/m)*(np.dot(x.T,(h-y)))\n",
    "  \n",
    "    J = float(J)\n",
    "    return J,theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training our model\n",
    "Now we have everything we need. We have a method to generate the input vector that we want and we have a way of determining the optimal values for our theta (allowing us to predict).\n",
    "The only thing that's missing is training our model. For this, I will split our data into training and test and proceed to fit the model and calculate the optimal values that we'll later use to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running gradient descent....\n",
      "J:0.14125806473785132, theta:[[ 2.23722850e-07]\n",
      " [ 9.74921055e-04]\n",
      " [-8.94219667e-04]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Let's split our data into train and test\n",
    "all_tweets = all_positive_tweets + all_negative_tweets\n",
    "# Generate a matrix with our labels\n",
    "labels = np.append(np.ones((len(all_positive_tweets),1)), np.zeros((len(all_negative_tweets),1)))\n",
    "\n",
    "# Split our data into train and test data. Take 8000 as train and 2000 for test\n",
    "x_train, x_test, y_train, y_test = train_test_split(all_tweets,labels,test_size=0.2)\n",
    "\n",
    "# Generate our dictionary with the frequencies for the training data\n",
    "frequencies_dict = frequency_dictionary(x_train,y_train)\n",
    "\n",
    "# Reshape y_train so it's a matrix rather than a list\n",
    "y_train = np.reshape(y_train, (len(y_train),1))\n",
    "\n",
    "# Extract features for all of our x_train tweets\n",
    "x = np.zeros((len(x_train), 3))\n",
    "for i in range(0,len(x_train)):\n",
    "    x[i:] = extract_features(x_train[i],frequencies_dict) \n",
    "\n",
    "# Hyperparameters\n",
    "weights = np.zeros((3,1)) # PARAMETERS WE WANT TO LEARN\n",
    "learning_rate = 1e-9\n",
    "num_iterations = 5000\n",
    "print(\"Running gradient descent....\")\n",
    "J, theta = gradientDescent(x,y_train,weights,learning_rate,num_iterations)\n",
    "\n",
    "print(f\"J:{J}, theta:{theta}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing our logistic regression model\n",
    "Let's create a method that can help us predict the sentiment of a given tweet. Now we have the parameters we need (we obtained them just before), so we can use them to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tweet(tweet, theta, freqs):\n",
    "    '''\n",
    "    Input: \n",
    "        tweet: a string\n",
    "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label) for the training set\n",
    "        theta: (3,1) vector of weights, we obtained them before\n",
    "    Output: \n",
    "        y_pred: the probability of a tweet being positive or negative\n",
    "    '''\n",
    "    # Transform the tweet to the features vector\n",
    "    x = extract_features(tweet,freqs)\n",
    "    \n",
    "    # Apply the activation function we used before\n",
    "    z = np.dot(x,theta)\n",
    "    y_pred = sigmoid(z)\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am happy -> 0.536411\n",
      "I am bad -> 0.491138\n",
      "this movie should have been great. -> 0.529330\n",
      "great -> 0.529353\n",
      "great great -> 0.558505\n",
      "great great great -> 0.587259\n",
      "great great great great -> 0.615430\n"
     ]
    }
   ],
   "source": [
    "# Run this cell to test your function\n",
    "for tweet in ['I am happy', 'I am bad', 'this movie should have been great.', 'great', 'great great', 'great great great', 'great great great great']:\n",
    "    print( '%s -> %f' % (tweet, predict_tweet(tweet,theta,frequencies_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
