{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis - Logistic Regression\n",
    "In this small POC, I will show you how you can apply logistic regression in order to determine if a given tweet has a positive or a negative sentiment behind it.\n",
    "For this, we will use a dataset included in NLTK package that already contains 10k labeled tweets, 5k of them marked as positive and the other 5k as negative. This is really good, since we will have a perfect distribution of tweet so our model can learn better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data and understanding it\n",
    "Our first step is always getting along with the data, we need to understand what is the format of it and how we can obtain the information that we need to train our model with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of positive tweets: 5000\n",
      "Amount of negative tweets: 5000\n",
      "\n",
      "Some positive tweets:\n",
      "['@Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!', '@DespiteOfficial we had a listen last night :) As You Bleed is an amazing track. When are you in Scotland?!', '@97sides CONGRATS :)', 'yeaaaah yippppy!!!  my accnt verified rqst has succeed got a blue tick mark on my fb profile :) in 15 days', '@BhaktisBanter @PallaviRuhail This one is irresistible :)\\n#FlipkartFashionFriday http://t.co/EbZ0L2VENM', \"We don't like to keep our lovely customers waiting for long! We hope you enjoy! Happy Friday! - LWWF :) https://t.co/smyYriipxI\", '@Impatientraider On second thought, there’s just not enough time for a DD :) But new shorts entering system. Sheep must be buying.', 'Jgh , but we have to go to Bayan :D bye', 'As an act of mischievousness, am calling the ETL layer of our in-house warehousing app Katamari.\\n\\nWell… as the name implies :p.']\n",
      "\n",
      "Some negative tweets:\n",
      "[\"Everything in the kids section of IKEA is so cute. Shame I'm nearly 19 in 2 months :(\", '@Hegelbon That heart sliding into the waste basket. :(', '“@ketchBurning: I hate Japanese call him \"bani\" :( :(”\\n\\nMe too', 'Dang starting next week I have \"work\" :(', \"oh god, my babies' faces :( https://t.co/9fcwGvaki0\", '@RileyMcDonough make me smile :((', '@f0ggstar @stuartthull work neighbour on motors. Asked why and he said hates the updates on search :( http://t.co/XvmTUikWln', 'why?:(\"@tahuodyy: sialan:( https://t.co/Hv1i0xcrL2\"', 'Athabasca glacier was there in #1948 :-( #athabasca #glacier #jasper #jaspernationalpark #alberta #explorealberta #… http://t.co/dZZdqmf7Cz']\n"
     ]
    }
   ],
   "source": [
    "# Import some libraries we always use\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk.corpus import twitter_samples\n",
    "\n",
    "\n",
    "# To know the ids of the json files, we can run the following command\n",
    "#print(twitter_samples.fileids())\n",
    "\n",
    "# Reading our files\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "\n",
    "# Count how many type of tweets we have in each case\n",
    "print(f\"Amount of positive tweets: {len(all_positive_tweets)}\")\n",
    "print(f\"Amount of negative tweets: {len(all_negative_tweets)}\")\n",
    "print()\n",
    "\n",
    "# Let's display some samples\n",
    "print(\"Some positive tweets:\")\n",
    "print(all_positive_tweets[1:10])\n",
    "print()\n",
    "print(\"Some negative tweets:\")\n",
    "print(all_negative_tweets[1:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing and cleaning our data\n",
    "This is a classic step of the NLP pipeline. In here, we will proceed to clean the text, remove punctuation, tokenize it into separate words, remove stopwords (or most common words in English) and transform them into their root version (stemming).\n",
    "\n",
    "For this particular case of tweets, we can see that there are some URLS inside of the message that we should also get rid of (since URLS don't add any value in the sentiment of a message). Same thing happens with quotes or tags (we can see in the previos messages some mentions to users such as \"@ketchBurning\" which won't add any sentiment either).\n",
    "Let's get rid of all this information and clean our text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Tweet: #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
      "Processed Tweet: ['followfriday', 'top', 'engag', 'member', 'commun', 'week', ':)']\n"
     ]
    }
   ],
   "source": [
    "import re                                  # library for regular expression operations\n",
    "import string                              # for string operations\n",
    "\n",
    "from nltk.corpus import stopwords          # module for stop words that come with NLTK\n",
    "from nltk.stem import PorterStemmer        # module for stemming\n",
    "from nltk.tokenize import TweetTokenizer   # module for tokenizing strings\n",
    "\n",
    "ps = PorterStemmer()\n",
    "en_stopwords = stopwords.words('english')\n",
    "\n",
    "def process_tweet(tweet):\n",
    "    # Remove hashtag, retweet marks, and hyperlinks\n",
    "    # Remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "\n",
    "    # Remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "\n",
    "    # Remove hashtags\n",
    "    # Only removing the hash # sign from the word\n",
    "    tweet = re.sub(r'#', '', tweet)  \n",
    "    \n",
    "    # Tokenize and lowercase words so (\"Hello\" and \"hello\" have the same meaning)\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,reduce_len=True)\n",
    "    tweet_tokenized = tokenizer.tokenize(tweet)\n",
    "    \n",
    "    # Remove punctuation, stopwords and stem token\n",
    "    tweet_tokenized_cleaned = []\n",
    "    for token in tweet_tokenized:\n",
    "        if token not in string.punctuation and token not in en_stopwords:\n",
    "            tweet_tokenized_cleaned.append(ps.stem(token))\n",
    "    \n",
    "    return tweet_tokenized_cleaned\n",
    "    \n",
    "\n",
    "# Testing it out\n",
    "sample_tweet = all_positive_tweets[0]\n",
    "processed_tweet = process_tweet(sample_tweet)\n",
    "print(f\"Sample Tweet: {sample_tweet}\")\n",
    "print(f\"Processed Tweet: {processed_tweet}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What can we use to train our model with?\n",
    "Well, we now have the capability of processing each one of our tweets. However, we need to think what type of information we're going to use in our model in order for it to determine if a given token has a positive or a negative sentiment.\n",
    "\n",
    "One thing that is clear is that we want a vector representation for each one of our tweets, and we want that representation be as optimal as possible. This is called feature extraction.\n",
    "What we could do in this case is start by counting how many times each word appears in possitive tweets and how many times it appears in negative ones. We will then generate a method that can give us a dictionary with (word,label) as keys and then as value we will have the total times that word appeared in messages with that label.\n",
    "\n",
    "As an example, we might have:\n",
    "(\"week\", 1) -> 15\n",
    "(\"week\", 0) -> 5\n",
    "Meaning that the token \"week\" appeared 15 times in possitive messages (that's why we use 1 as part of the key) and 5 times in tweets labeled as negative. We will use this dictionary to later build our features vector (this is, the vector we will use as input to our model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as input, this will receive all the tweets and labels (1 or 0) we use to train our model with\n",
    "def frequency_dictionary(tweets,labels):  \n",
    "    # I will zip the tweets and the labels so we can get a tuple representation of each tweet and his value\n",
    "    tweets_labels = zip(tweets,labels)\n",
    "    freq_dict = {}\n",
    "    \n",
    "    for (tweet,label) in tweets_labels:\n",
    "        tweet_tokens = process_tweet(tweet)\n",
    "        for token in tweet_tokens:\n",
    "            pair = (token,label)\n",
    "            if pair in freq_dict:\n",
    "                freq_dict[pair] +=1\n",
    "            else:\n",
    "                freq_dict[pair] = 1\n",
    "                \n",
    "# Let's try this with 50 messages from each \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
